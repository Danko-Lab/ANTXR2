#!/usr/bin/env python

# # SweepFinder2 software
# 
# The manual for SweepFinder2 is here: http://www.personal.psu.edu/mxd60/Manual_SweepFinder2_v1.0.pdf
# 
# Following the manual and the requirements for the input files will lead you to the same set of files as I.
# 
# ## Necessary input files:
# 
# 1. An allele frequency file (.af). This must be generated from individuals from the population of interest. A simple process is described to generate these files.
# 2. A background selection model file (.bkgd; from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2669884/). This file HAS to be reformatted into the proper input for SF2. 
# 3. A recombination map (.rmap; from Decode: https://www.decode.com/addendum/) This file HAS to be reformatted into the proper input for SF2.
# 4. An empirical allele frequency spectrum this generated by SF2 commands. 
# 
# I present below a brief description of the tools and software used to create these input files you DO NOT have to recreate the input files to run SweepFinder2 this is merely for clarity and for reproducibility. In fact I recommend not recreating these files as they are tedious to run and reformat.

# ## 1) Generate .af file
# 
# In order to do this we must first obtain a VCF file for our chromosome of interest. In this case from 1k genomes and the list of individuals from each population from 1k genomes as well. Each vcf for the populations can be found here:
# 
#     /local/workdir/Iskander/POP_VCFS/
#     
# 
# 
# But to create a vcf  for each population you would like to run SF2 on by do the following example:
# 
#     grep CEU integrated_call_samples_v3.20130502.ALL.panel.txt | cut -f1 > CEU.samples.list
#     vcf-subset -c CEU.samples.list ALL.chr4.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz | fill-an-ac | bgzip -c > CEU.chr4.phase3.vcf.gz
#     
# Simply replace the population name with the one you would like to run SF2 on. Once you have the VCF you must convert the vcf to a .counts file to more easily convert it to a .af file using my script convertAF.py. Use the following command:
# 
#     vcftools --gzvcf filename.vcf.gz --counts --out outfile.frq.counts
#     python convertAF.py outfile.frq.counts
# 
# The .counts file are in the directory:
#      
#      /local/workdir/Iskander/ALLELE_COUNTS/
# 
# With the designation .chr4.phase3.frq.count and the final .af files are in the directory:
# 
#     /local/workdir/Iskander/SF2_INPUTS/
#     
# 
# ### NOTES AND KNOWN BUGS:
# 
# #### Notes:
# To fully complete the construction of the .af files you must have the same number of fields in these files as you do in the .bkgd and .rmap files. So you must make sure that there is ONE and only ONE entry in the .rmap and .bkgd files for each entry in the .af file and vice versa. Of course it is optional to use background selection model or a recombination map, but this is a bad idea.
# 
# #### Bugs:
# The current af file converter is not handling indels or SNPs that are greater than 2 alleles. This means that this considers indels as a regular polymorphism and triallelic sites as biallelic sites by getting one of the minor alleles as the allele frequency. Also none of the alleles are being called as polarized -- the ancestral allele is not noted. Updating the program to exclude indels, and triallelic sites would reduce the number of markers but would generally be a good idea. I think these are probably rare enough that they haven't effected my  sweep estimates by too much. However, the ancestral allele is a relevant parameter that I have, admittedly, overlooked. This will probably make our computing faster and more accurate, hopefully. There are relevant vcftools paramaters to fix these.

# # 2) Generate .bkgd file
# 
# The background map files taken from Mcvicker et al. are not formatted in a convenient way nor are they in the hg19 release. It was a slightly tedious process to reformat them into a usable format and the extended details of how I did this can be seen in my notes for 'Parsing bmap and rmap files' in my rotation Jupyter notebook referenced in the introduction. The take home is that I had to convert the coordinates to hg19 release and turn the intervals from the original file into the SNP positions from the .af file. In addition not all regions were succesfully converted which made things tedious. The .bkgd files for chr4 have been created already and the file is in:
# 
#     /local/workdir/Iskander/SF2_INPUTS/
# 
# With the designation .bkgd. The original .bkgd files that were used to create this 'pruned' version are in the directory.
# 
#     /local/workdir/Iskander/bkgd/
#     
# I do not recomend remaking the bkgd file the current version contains all polymorphisms that were called from the VCF if you wish to limit the .af file to only biallelic and no indels then you should simply remove the fields that are filtered from the .af from the .bkgd. 
# 
# 

# # 3) Generate .rmap file
# 
# The recombination map file from Decode had to similarly be reformatted like the .bkgd file. In an equally tedious way and I will reiterate that recreating this file is an equally horrible process that requires converting the coordinates to hg19 and calling SNPs that are between intervals using laborious pairwise comparisons. More details on how I did this can be found in my Jupyter rotation lab notebook. The rmap file is here:
# 
#     /local/workdir/Iskander/SF2_INPUTS/
# 

# ## NOTES ON GENERATING NEW RMAP AND BKGD
# If you absolutely MUST create another .bkgd file or rmap then you must perform the following:
# 
# 1. Reformat the .bkgd file to a bed file.
# 2. Use UCSC genome browser liftOver to convert the bed file from hg18 to hg19.
# 3. Turn the bed file back into a SF2 interpretable .bkgd file using the following python script:
# 
#     python make_bkgd_rmap.py --bkgd file1.bed --rmap file2.bed --allele_freq snps.af --out output_names
#     
#     The reason that this process is not directly coupled with the below script is that this is an incredibly time intesive task. It does many pairwise comparisons and so it is most efficient to split the af file into many smaller files and run the scripts in parallel and then recat all the files together.
#     
# 4. Now we must create a pruned version of these 3 files where they contain the same number of marker SNPs in each file. To do this use the following script:
# 
#     python prune_sf2_inputs.py file1.bkgd file2.rmap file3.af outputName
#     
#     The outputs from the last scripts will be complete, but should be checked by making sure each file has the same length.

# # 4) Generate .spct file
# 
# This is the simplest of the files to generate. Simply call:
# 
#     ./SweepFinder2 â€“f CombinedFreqFile SpectFile 
#     
# Where the CombinedFreqFile is the .af file generated. It is suggested that the CombinedFreqFile be generated by using the whole genome data, but I found that this process would be very laborious and would take longer than the scope of my rotation. To be completely thorough it may be wise to compute the genome wide allele frequency spectrum, but I doubt it would alter our results too much. The generated .spct files are in the following directory:
# 
#         /local/workdir/Iskander/SF2_INPUTS/SPECTRUM/

# # SweepFinder2 Output:
# 
# Now that we have generated the input files we can run the command to generate the output:
# 
#     SweepFinder2 -lrb 180000 input.chr4.SF2.pruned.af input.chr4.phase3.SF2.spct chr4.hg19.pruned.rmap chr4.hg19.pruned.bkgd 10000 99000 240000 output_lrb.sweep.out
#     
# Full detail on the parameters can be seen in the SweepFinder2 manual, but briefly -lrb designates the use of a recombination map and background selection map and the 180000 is the number of grid points calculated. The last three numbers were parameters obtained from Huber et al. (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5082542/) the parameters they used were as follows: N1 = 10000, N2 = 99000 and T = 240000. That is initial Ne of N1, final Ne of N2 and time in coalescent units as T. They performed similar sweep analysis on human data and so I believe these parameters may be reasonable although perhaps a more sophisticated human population geneticist could give you better numbers.
# 
# 
# ### Analysis
# 
# The process ran although it took roughly 2 weeks. I would recommend for future reducing the number of grid points to 1 every 5kb instead of 1 every 1kb. to speed it up. Here I will describe a simple function for analyzing this data. The output files are in: 
#     
#     /local/workdir/Iskander/SF2_OUTS/

# In[5]:


import matplotlib.pyplot as plt
import csv
import numpy as np
import seaborn as sns
import os

def read(fileName, path):
    fullPATH = os.path.join(path, fileName)
    position = []
    LR = []
    alpha = []
    with open(fullPATH, 'r') as myCLR:
        reader = csv.reader(myCLR, delimiter='\t')
        next(reader)
        for line in reader:
            position.append(float(line[0]) / 1000000)

            LR.append(float(line[1]))
            alpha.append(float(line[2]))

    return LR, position

def plotSweep(positions, LRs, ylim=False, xlim=False, title='SweepFinder2 CLR Estimates'):

    legends= ['CEU', 'CHB', 'JPT', 'YRI', 'rs41407844']
    #sig = np.percentile(LR, 95)

    fig = plt.figure(figsize=(15, 10))
    # axs = fig.subplots(2,1, sharex=True)

    #plt.axvline(80.898662, color='blue', linestyle='--')
    #
    #plt.axhline(sig, color='purple', linestyle='--')

    for output in range(4):
        sns.lineplot(positions[output], LRs[output])
    plt.axvline(81.000953, color='red', linestyle='--')
    plt.ylabel('Likelihood', fontsize=15)


    plt.xlabel('Position (Mb)', fontsize=15)
    plt.title('{0}'.format(title), fontsize=25)
    if xlim == False:
        pass
    else:
        plt.xlim((xlim[0], xlim[1]))
    if ylim == False:
        pass
    else:
        plt.ylim(ylim)
    plt.legend(legends)

    plt.show()

def main():
    LRs = []
    positions = []
    for file in os.listdir('/home/iskander/Documents/Danko_lab/sweeps/SF2_OUTPUTS'):
        if file.endswith('.out'):
            LR, pos = read(file, path='/home/iskander/Documents/Danko_lab/sweeps/SF2_OUTPUTS')
            LRs.append(LR)
            positions.append(pos)

    plotSweep(positions=positions, LRs=LRs, xlim=(81,81.06), ylim=(0,80))




# In[4]:


main()


# ## Closing remarks
# 
# A clear signal for CEU proximal to our SNP of interest and in a region rich with regulatory elements. This is a good signal for our hypothesis about CEU experiencing selection. Over all this tool seems to be a reasonable approach to sweep detection. Although preprocessing the files can be quite laborious hopefully these scripts will make future analyses of this type easier. I think that making the changes to the .af files to limit indels, etc. would be a very good idea and something that I overlooked initially. I would try to fix it myself, but if the initial runs of SF2 are any indication I will be done with my rotation long before the next run finishes. 

# # Fst
# 
# All Fst values were computed using VCFtools using the --weir-fst commands from the manual here: http://vcftools.sourceforge.net/man_latest.html. Using the vcf from 1k genomes
# 
# The .weir.fst files were generated by using the vcftools --weir-fst command and adjusting window size to 5kb and step size to 5kb as well to generate 5kb bins. The fst estimates for the snps were generated by simply using the --weir-fst command. Remember that Fst is statistic of differentiation between populations and so you must input the names of two population groups into the --weir-fst parameter.
# 
# 
# Fst was calculted for all pairwise comparisons of CEU, YRI, LWK, JPT and CHB, AND Hadza, Sandawe and Pygmy Baka. The files are in the following directory:
# 
#     /local/workdir/Iskander/POP_VCFS/FST/
#     
# The 5kb binned estimates of fst have the extensions _windows.windowed.weir.fst and the Fst calculations of each SNP have the extension _snp.weir.fst. 

# ## Plotting Bins:
# 
# To plot the PW comparisons I have generated the script below to easily generate permutations of population comparisons:

# In[81]:


import csv
import matplotlib.pyplot as plt
import numpy as np
import os

def pwFST(path, pop, samples, xlim=False, extension='_windows.windowed.weir.fst', ylim= False):
    colors = ['red', 'green', 'blue', 'orange', 'purple', 'black', 'yellow' ]
    color_id = 0

    plt.figure(figsize=(20,5))
    #samples = list(set(samples) - set([pop]))
    for comparison_pop in samples:
        
        file_desig = '{0}_{1}{2}'.format(pop, comparison_pop, extension)
        filePath = os.path.join(path, file_desig)
        binPositions = []
        mean_Fst = []
        with open(filePath, 'r') as myIN:
            fstReader = csv.reader(myIN, delimiter='\t')
            next(fstReader)
            for field in fstReader:
                binPositions.append((int(field[1]) + 2500)/1000000)#Plot bin midpoint
                
                mean_Fst.append(float(field[5]))
            myIN.close()

        
        plt.plot(binPositions, mean_Fst, color=colors[color_id])
   
        if xlim != False:
            plt.xlim(xlim)
            
        else:
            pass
        if ylim !=False:
            plt.ylim(ylim)
        
        color_id += 1
    plt.xlabel('Bin Midpoint Position (Mb)', fontsize=20)
    plt.axvline(80.898662, color='orange', linestyle='--')
    plt.axvline(81.000953, color='red', linestyle='--')

    plt.ylabel('Mean Fst', fontsize=20)

    #axs[2].set_ylabel('Number of SNPs')
    plt.title('{0} Fst 5kb Windows'.format(pop), fontsize=25)
    samples = samples+ ['Antxr2', 'rs41407844']
    plt.legend(samples, fontsize=12.5)
    plt.show()
            
            


# In[82]:


pwFST(path='/home/iskander/Documents/Danko_lab/sweeps/FST_Estimates/Windowed/', pop='CEU', samples=['YRI','JPT','CHB', 'LWK' ], xlim=(80.85,81.2), ylim=(0,.3))
pwFST(path='/home/iskander/Documents/Danko_lab/sweeps/FST_Estimates/Windowed/', pop='YRI', samples=['CEU','JPT','CHB', 'LWK', ], xlim=(80.85,81.2), ylim=(0,.3))


# In[83]:


pwFST(path='/home/iskander/Documents/Danko_lab/sweeps/FST_Estimates/Windowed/', pop='CEU', samples=['Hadza', 'Sandawe', 'PygmyBaka'], xlim=(80.85,81.2))
pwFST(path='/home/iskander/Documents/Danko_lab/sweeps/FST_Estimates/Windowed/', pop='YRI', samples=['Hadza', 'Sandawe', 'PygmyBaka'], xlim=(80.85,81.2), ylim=(-.1,.5))


# There are peaks of mean Fst near the rs40407844 locus that is unique to all CEU Fst comparisons. This especially obvious when comparing Hunter Gatherers to CEU.
# 
# 
# ## Fst Enrichment
# We will now plot the Fst enrichment of each window, where enrichment is defined as follows: The integral from the Fst value of the window to the tail of the distribution. Where the distribution is calculated by approximating a Gaussian KDE on the dataset of mean Fst values of each window. The script used to compute the Fst enrichment is:
# 
#     integrate_windows.py
#     
# The files generated from this script are .p files that have the Fst enrichment as a column values for each bin start point instead of Fst. These .p files are in the directory:
# 
#     /local/workdir/Iskander/P_VAL/
# 
#     

# In[84]:


import csv
import matplotlib.pyplot as plt
import numpy as np
import os

def p_value_windows(path, pop, samples=['EUR', 'EAS', 'AFR'], xlim=False, extension='_windows.windows.p', ylim= False):
    colors = ['red', 'green', 'blue', 'orange', 'purple', 'black', 'yellow' ]
    color_id = 0

    plt.figure(figsize=(20,5))
    #samples = list(set(samples) - set([pop]))
    for comparison_pop in samples:
        
        file_desig = '{0}_{1}{2}'.format(pop, comparison_pop, extension)
        filePath = os.path.join(path, file_desig)
        binPositions = []
        mean_Fst = []
        with open(filePath, 'r') as myIN:
            fstReader = csv.reader(myIN, delimiter='\t')
            next(fstReader)
            for field in fstReader:
                binPositions.append((int(field[0]) + 2500)/1000000)
                
                mean_Fst.append(-np.log(float(field[1])))
            myIN.close()


        plt.plot(binPositions, mean_Fst, color=colors[color_id])
     
        #axs[2].plot(binPositions, N_variants, color='blue')
        if xlim != False:
            plt.xlim(xlim)
            
        else:
            pass
        if ylim !=False:
            plt.ylim(ylim)
        
        color_id += 1
    plt.xlabel('Bin Midpoint Position (Mb)', fontsize=20)
    plt.axvline(80.898662, color='orange', linestyle='--')
    plt.axvline(81.000953, color='red', linestyle='--')
    #plt.axhline(-np.log(.05), color='black' , linestyle='--')
    plt.ylabel('Fst Enrichment (-log(p))', fontsize=20)

    #axs[2].set_ylabel('Number of SNPs')
    plt.title('{0} Windows'.format(pop), fontsize=25)
    samples = samples+ ['Antxr2', 'rs41407844']
    plt.legend(samples, fontsize=12.5)
    plt.show()
            
            


# In[85]:


p_value_windows(path='/home/iskander/Documents/Danko_lab/sweeps/FST_Estimates/P_VAL/', pop='CEU', samples=['YRI', 'JPT', 'CHB','LWK'], xlim=(80.85,81.06), ylim=(0,7))
p_value_windows(path='/home/iskander/Documents/Danko_lab/sweeps/FST_Estimates/P_VAL/', pop='YRI', samples=['CEU', 'JPT','CHB', 'LWK'], xlim=(80.85,81.06), ylim=(0,7))


# The plots of Fst enrichment are notably similar to the plots of mean Fst as they should recapitulate that data. But this will display how extreme any given mean Fst is. You can see that in CEU that near our SNP locus there are some windows that become more enriched for Fst, while the opposite is true in the YRI comparisons. In both YRI and CEU however there is an enrichment for Fst against east Asians in the area between Antxr2 and rs414.
# 
# If we were to know calculate the exact value of Fst enrichment for the exact window around our rs41407844 we will get the following by using a simple script from below:
# 
# 

# In[70]:


import os
import csv
import seaborn as sns
import matplotlib.pyplot as plt
import operator
import scipy.stats as stats

def readFile(inFile, path='/home/iskander/Documents/Danko_lab/sweeps/FST_Estimates/Windowed/'):
    mean_fst = []
    with open(os.path.join(path, inFile), 'r') as myInput:
        read_input = csv.reader(myInput, delimiter='\t')
        next(read_input)
        for field in read_input:
            mean_fst.append(float(field[5]))
        myInput.close()
    
    return mean_fst

def compute_pval(FST, SNP, kde):
    
    return kde.integrate_box(SNP, max(FST))

def kde_function(fst_values):#Create a gaussian kernel density estimate PDF from which to calculate the likelihoods
    
    kde = stats.gaussian_kde(fst_values)
    
    return kde

def plot_KDE(SNP, empirical_distr, p, name='Density'):
    kde_max = np.argmax(stats.gaussian_kde(empirical_distr))
    
    sns.kdeplot(empirical_distr, shade=True)
    plt.xlabel('Fst')
    plt.ylabel('Density')
    plt.title(name)
    plt.axvline(SNP, color='red', linestyle='--')
    plt.text(max(empirical_distr)/2.5,2 ,'Fst of window more extreme than\n {0:0.2f}% of empirical distr.'.format((1-p)*100))
    plt.show()
    


# In[71]:


all_files = [('JPT_CHB_windows.windowed.weir.fst', 0.000501977543333), ('CEU_JPT_windows.windowed.weir.fst', 0.102860030963), ('CEU_CHB_windows.windowed.weir.fst', 0.107969525964), ('CEU_YRI_windows.windowed.weir.fst', 0.132479333553), ('YRI_CHB_windows.windowed.weir.fst', 0.0332899716222), ('YRI_JPT_windows.windowed.weir.fst', 0.0390497059091)] 
#File name, mean Fst of window
#Each of the mean Fst values of the windows around the SNP were calculated before hand in my notes in the Detecting_Selective_Sweeps books
#You may also recalculate this value by very simply reading in all SNPs in a 5kb window around the SNP and taking the mean
def run_all(file, target_mean):
    title = file.split('_')[0]+' v. '+file.split('_')[1] + ' Chr4 Fst'
    fst_arr = readFile(file)
    
    gauss = kde_function(fst_arr)
    p = compute_pval(fst_arr, target_mean, gauss)
    plot_KDE(SNP=target_mean, empirical_distr=fst_arr, p=p, name=title)
    #print('{0}\t{1}'.format(file, p))

for obj in sorted(all_files, key=operator.itemgetter(0)):
    run_all(obj[0], obj[1])
    


# ## Closing remarks 
# 
# The windows surrounding the SNP of interest are highly differentiated in CEU residing in the 94-98 percentile of mean Fst. This is a good sign that this SNP or region is causative. The plotting of the windows shows a much weaker signal of mean Fst generally, or at least a more difficult to display difference. The density plots I think are much stronger evidence, at least graphically, of a CEU specific population difference. In any case Fst signals are not incredibly strong by population genetics standards, but perhaps in the human population genetics world this is stronger evidence. 
# 
# Playing with the window size may be informative as I settled on 5kb semi-arbitrarily. I wanted to find a window size that would maximize the difference in Fst of populations around our SNP of interest and so optimized it for that purpose. 

# ## SNP Fst
# 
# Fst was also calculated for all markers from the VCF. First I will show differentiated our SNP of interest is relative to the rest of Fst values in the distribution. Then I will show a manhattan plot of the Fst enrichment of each SNP in our interesting region.

# In[143]:


import os
import csv
import scipy.stats as stats
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def KDE(inFile, path, SNP=81000953):
    fullPATH = os.path.join(path, inFile)
    fstArray = []
    with open(fullPATH, 'r') as myFile:
        fst = csv.reader(myFile, delimiter='\t')
        next(fst)
        for field in fst:
            fstArray.append(max(float(field[2]),0))
            if int(field[1]) == SNP:#Routine assigns the Fst of SNP of interest to an object
                snp_fst = float(field[2])
                
            else:
                pass
            
        myFile.close()
    
    
    kde = stats.gaussian_kde(fstArray)
    pval = kde.integrate_box(snp_fst, max(fstArray))
    plt.text(max(fstArray)/4, 5, 'SNP in {0:.2f} percentile'.format((1-pval)*100))

    sns.kdeplot(fstArray, shade=True)
    plt.ylabel('Density')
    plt.xlabel('Fst')
    plt.axvline(snp_fst, linestyle='--', color='red')
    plt.title('{0} vs. {1} Fst'.format(inFile.split('_')[0],inFile.split('_')[1]))
    plt.legend(['rs41407844'])
    plt.show()
        


# In[144]:


KDE('CEU_YRI_snp.weir.fst', '/home/iskander/Documents/Danko_lab/sweeps/FST_Estimates/SNP/')
KDE('CEU_LWK_clean_snp.weir.fst', '/home/iskander/Documents/Danko_lab/sweeps/FST_Estimates/SNP/')
KDE('CEU_JPT_snp.weir.fst', '/home/iskander/Documents/Danko_lab/sweeps/FST_Estimates/SNP/')
KDE('CEU_CHB_snp.weir.fst', '/home/iskander/Documents/Danko_lab/sweeps/FST_Estimates/SNP/')


# In[45]:


KDE('YRI_JPT_snp.weir.fst', '/home/iskander/Documents/Danko_lab/sweeps/FST_Estimates/SNP/')
KDE('YRI_CHB_snp.weir.fst', '/home/iskander/Documents/Danko_lab/sweeps/FST_Estimates/SNP/')


# Our SNP is in the 98-99 percentile in all CEU Fst calculations but is in the 85th percentile for YRI vs. East Asians. 
# 
# I have calculated the Fst enrichment of each SNP by the methods describe in the Fst windows section using the script: 
# 
#     fst2pvalue.py
#     
# These .p files are in the directory:
#     
#     /local/workdir/Iskander/P_VAL/ 
#     
# under the extension _snp.p. These .p files were used to generate Manhattan plots of the Fst enrichment and were used to find candidate SNPs for genetic screens.

# In[145]:


import rpy2.rinterface
get_ipython().run_line_magic('load_ext', 'rpy2.ipython')


# In[152]:


get_ipython().run_cell_magic('R', '', "\n#####\nlibrary(qqman)\nsetwd('~/Documents/Danko_lab/sweeps/FST_Estimates/P_VAL')\nceu_yri_p = read.table('CEU_YRI_snp.v2.p', header=TRUE)\nmanhattan(ceu_yri_p, chr='CHROM', bp='POS', snp='POS', p='PVALUE', main='CEU_YRI Fst Enrichment', xlim=c(80900000,81060000), highlight='81000953')\nabline(h=-log10(.05), col='purple')\n\n#####\nyri_jpt_p = read.table('YRI_JPT_snp.v2.p', header=TRUE)\nmanhattan(yri_jpt_p, chr='CHROM', bp='POS', snp='POS', p='PVALUE', main='YRI_JPT Fst Enrichment', highlight='81000953', xlim=c(80900000,81060000))\nabline(h=-log10(.05), col='purple')\n\n####\n###\njpt_chb_p = read.table('JPT_CHB_snp.v2.p', header=TRUE)\nmanhattan(jpt_chb_p, chr='CHROM', bp='POS', snp='POS', p='PVALUE', main='JPT_CHB Fst Enrichment', highlight='81000953', xlim=c(80900000,81060000))\nabline(h=-log10(.05), col='purple')")


# ## Closing remarks:
# 
# Our candidate SNP is certainly highly differntiated, but there are clusters of SNPs in the CEU Fst comparisons near our candidate loci. But unfortunately there are also a large cluster of highly differentiated SNPs in these YRI east asian comparisons. However, if we take the SNP data with the mean Fst windows then it would seem that our candidate region in CEU has a higher proportion of Fst enrichment to unenriched SNPs. 

# # Pi
# 
# Pi was calculated for each population by using vcftools using the command:
# 
#     vcftools --gzvcf pop.vcf.gz --window-pi 5000 --window-pi-step 5000 
# 
# This produced 5kb non-overlapping windows. The files are in the directory:
# 
#     /local/workdir/Iskander/POP_VCFS/Pi
#     
# There was also SNP Pi data generated that is in the same directory under the .sites.pi extension in the same directory. The analysis and visualization of the 5kb bins was done with the below script.

# In[ ]:





# In[136]:


def windowPi(inFile):
    pos = []
    pi = []
    path = '/home/iskander/Documents/Danko_lab/sweeps/Pi/'
    fullPath = os.path.join(path, inFile)
    with open(fullPath, 'r') as myIN:
        readIN = csv.reader(myIN, delimiter='\t')
        next(readIN)
        for field in readIN:
            pos.append((int(field[1])+2500) / 1000000)
            pi.append(float(field[-1]))
            
    return pos, pi

def plotWin(pos, pi, title='Windowed Pi', xlim = False, ylim = False):
    plt.figure(figsize=(20,5))
    avg = np.average(pi)
    fifth = np.percentile(pi, 5)
    plt.axhline(avg, color='red', linestyle='--')
    plt.axhline(fifth, color='purple', linestyle='--')
    plt.xlabel('Bin Midpoint Position (Mb)', fontsize=15)
    plt.ylabel('Pi', fontsize=15)
    plt.plot(pos, pi)
    
    plt.axvline(81.000953, color='orange', linestyle='--')
    plt.title(title, fontsize=20)
    if xlim ==False:
        pass
    else:
        plt.xlim(xlim)
    if ylim == False:
        pass
    else:
        plt.ylim(ylim)
    handles = ['Chromosome wide average', '5th Percentile', title, 'rs41407844']
    plt.legend(handles, fontsize=15)
    plt.show()


# In[137]:


positions, pi_win = windowPi('CEU.windowed.pi')
plotWin(positions, pi_win, xlim = (80.9, 81.1), ylim = (0,.004), title='CEU 5kb Windows')
positions, pi_win = windowPi('CHB.windowed.pi')
plotWin(positions, pi_win, xlim = (80.9, 81.1),ylim = (0,0.004), title='CHB 5kb Windows')
positions, pi_win = windowPi('JPT.windowed.pi')
plotWin(positions, pi_win, xlim = (80.9, 81.1), ylim = (0,0.004), title='JPT 5kb Windows')


# In[118]:


positions, pi_win = windowPi('YRI.windowed.pi')
plotWin(positions, pi_win, xlim = (80.9, 81.1), ylim = (0,0.004), title='YRI 5kb Windows')
positions, pi_win = windowPi('LWK.windowed.pi')
plotWin(positions, pi_win, xlim = (80.9, 81.1), ylim = (0,0.004), title='LWK 5kb Windows')


# The analysis of the Pi bins shows a decrease of Pi in a region similar to the one called by SweepFinder2 in CEU, CHB and JPT. This is a reasonable result as there site frequency spectrum is related to nucleotide diversity. In CEU however there is less nucleotide diversity proximal to the SNP of interest that is not seen in the other populations. This seems like more reasonable evidence that there is in fact selection acting around this region of regulatory elements.

# # Tajima's D
# 
# Tajima's D files were created with vcftools as well by using the command:
# 
#     vcftools --gzvcf input.vcf.gz --TajimaD 5000
#     
# This will create .tajima.d files in 5kb bins. The files can be found in the following directory:
# 
#     /local/workdir/Iskander/POP_VCFS/TajimasD/
#     
# The below script will visualize the data.
# 

# In[138]:


import csv
import os
import matplotlib.pyplot as plt


def TD(file, xlim=False, title="Tajima's D"):
    path = '/home/iskander/Documents/Danko_lab/sweeps/TajimaD/'

    plt.figure(figsize=(20,5))
    legends = []
    for inFile in file:
        pos_array = []
        td_array = []
        full_path = os.path.join(path, inFile)
        with open(full_path, 'r') as myT:
            t_reader = csv.reader(myT, delimiter='\t')
            next(t_reader)
            for field in t_reader:
                position = (int(field[1])+2500)/1000000
                TD = float(field[3])
                pos_array.append(position)
                td_array.append(TD)
        myT.close()

        plt.plot(pos_array, td_array)

        legends.append(inFile.split('.')[0])
    if xlim != False:
        plt.xlim(xlim)
    else:
        pass
    
    plt.title("{0}".format(title), fontsize=20)
    plt.xlabel('Bin Midpoint Position (Mb)', fontsize=15)
    plt.ylabel("Tajima's D", fontsize=15)
    plt.axvline(81.000953, color='red', linestyle='--')
    legends = legends + ['rs41407844']
    plt.legend(legends, fontsize=15)
    plt.show()     
            


# In[142]:


TD(['CEU.5kb.Tajima.D','JPT.5kb.Tajima.D', 'CHB.5kb.Tajima.D', 'LWK.Tajima.D', 'YRI.5kb.Tajima.D'] , xlim=(80.95, 81.05), title='All Populations')
TD(['LWK.Tajima.D', 'YRI.5kb.Tajima.D'], xlim=(80.95,81.05), title='Africans')
TD(['Sandawe.Tajima.D', 'PygmyBaka.Tajima.D'], xlim=(80.95,81.05), title='Hunter Gatherers')


# ## Closing remarks
# 
# Tajima's D should not be over interpreted as without a strong demographic model it is easy to conflate demographic changes with selection. Both effect Tajima's D and can be misconstrued for each other. I think it is enocuraging, however, CEU, JPT and CHB show negative trends of Tajima's D proximal to the locus that we are interested in. This is probably in large part to the reduction of nucleotide diversity seen by the Pi analysis. The take home is that we should not be over interpreted, but MAY be a good sign.

# # Candidate SNP detection
# 
# With the 81-81.05 Mb region as being called under selection we can try and detect candidate SNPs by looking at the regulatory sequence data and finding which SNPs are highly differentiated. I have written a simple script that will take in one or more .p files (Fst enrichment data) and generate an intersection of the enriched SNPs. Then it will take as an argument a .counts file that contains the allele frequency of our population of interest and a bed file that contains regulatory information. The program will take the SNPs that are called as enriched and check every regulatory region in the bed file and return an allele and the frequency of each allele. The script that does this is:
# 
#     extract_fst.py
#     
# And a usage example is the following:
# 
#     python extract_fst.py reg_file.bed alleles.frq.counts >> output.txt
#     
# The default of this program will extract the alleles for CEU, but minor tweaking of the guts of this program will allow you to extract alleles of any population. It also will only extract high Fst snps from the 81 - 81.06 Mb region roughly called by SweepFinder2. This range can also be modified by tweaking a simple if/else statement in the readP() method. The filter of the Fst enrichment is also SNPs that are in the 95th percentile of enrichment. This can also be modified very easily within the script by modifying an if/else statement.
# 


